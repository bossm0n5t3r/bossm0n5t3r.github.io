+++
date = 2024-10-01T18:00:00+09:00
title = "Article Weekly, Issue 39"
authors = ["Ji-Hoon Kim"]
tags = ["Article", "Weekly"]
categories = ["Article", "Weekly"]
series = ["Article Weekly"]
+++

## Period

`2024-09-22` ~ `2024-09-28`

## Introducing Contextual Retrieval

- AI 모델이 특정 컨텍스트에서 유용하려면 배경 지식에 접근할 필요가 있음
- 개발자들은 일반적으로 Retrieval-Augmented Generation(RAG)를 사용해 AI 모델의 지식을 향상시킴
- 전통적인 RAG 솔루션은 정보를 인코딩할 때 컨텍스트를 제거하여 관련 정보를 검색하지 못하는 경우가 많음
- Contextual Retrieval은 RAG의 검색 단계를 크게 개선하는 방법으로, Contextual Embeddings과 Contextual BM25라는 두 가지 하위 기술을 사용함
- 이 방법은 검색 실패율을 49% 감소시킬 수 있으며, 재순위화와 결합하면 67%까지 감소시킬 수 있음
- 개발자는 Claude와 함께 자신만의 Contextual Retrieval 솔루션을 쉽게 배포할 수 있음
- RAG 기본: 대규모 지식 베이스로 확장
  - 컨텍스트 윈도우에 맞지 않는 더 큰 지식 베이스의 경우 일반적인 솔루션은 RAG임
  - RAG는 지식 베이스를 더 작은 청크로 분해하고, 이를 벡터 임베딩으로 변환한 다음, 의미론적 유사성을 통해 검색할 수 있는 벡터 데이터베이스에 저장함
  - 임베딩 모델은 의미 관계를 포착하는 데 뛰어나지만 중요한 정확한 일치를 놓칠 수 있음
  - BM25는 정확한 단어나 구문 일치를 찾기 위해 어휘 매칭을 사용하는 랭킹 함수로, 고유 식별자나 기술 용어를 포함하는 쿼리에 특히 효과적임
  - RAG 솔루션은 다음 단계를 사용하여 임베딩과 BM25 기술을 결합함으로써 가장 적용 가능한 청크를 더 정확하게 검색할 수 있음:
    - 지식 베이스(문서의 "코퍼스")를 일반적으로 몇 백 토큰 이하의 더 작은 텍스트 청크로 분해
    - 이러한 청크에 대한 TF-IDF 인코딩과 의미론적 임베딩 생성
    - BM25를 사용하여 정확한 일치를 기반으로 상위 청크 찾기
    - 임베딩을 사용하여 의미론적 유사성을 기반으로 상위 청크 찾기
    - 순위 융합 기술을 사용하여 (3)과 (4)의 결과를 결합하고 중복 제거
    - 상위 K개 청크를 프롬프트에 추가하여 응답 생성
- Contextual Retrieval 도입
  - 전통적인 RAG에서는 문서를 더 작은 청크로 분할하기 때문에 개별 청크가 충분한 맥락을 결여할 수 있음
  - Contextual Retrieval은 각 청크 앞에 청크별 설명 컨텍스트를 추가하여 이 문제를 해결함
  - Claude는 전체 문서의 맥락을 사용하여 청크를 설명하는 간결한 청크별 컨텍스트를 제공하도록 지시하는 프롬프트를 사용하여 Contextual Retrieval 구현을 지원함
  - Contextual Retrieval은 프롬프트 캐싱 덕분에 Claude와 함께 저렴한 비용으로 이용 가능함
- 성능 개선
  - Contextual Embeddings는 상위 20개 청크 검색 실패율을 35% 감소시킴 (5.7% → 3.7%)
  - Contextual Embeddings와 Contextual BM25를 결합하면 실패율이 49% 감소함 (5.7% → 2.9%)

## Real-time Linux is officially part of the kernel after decades of debate

- Realtime Linux가 커널의 일부로 공식 포함되어, 이제 실시간 운영체제(RTOS) 없이 "실시간 리눅스" 사용 가능해 질 것
- 오디오 장비, 산업용 용접 레이저, 화성 탐사선 등에 "실시간 Linux"를 사용하고 싶다면 오래전부터 그 옵션이 있었음 (QNX 등의 대안을 사용하지 않는다는 가정 하에)
- 대학들이 1990년대 후반부터 자체 실시간 커널을 만들기 시작함
- PREEMPT_RT라는 패치 세트가 적어도 2005년부터 존재해 왔음
- NO_HZ와 같은 실시간 작업의 일부 측면은 오래전에 메인라인 커널로 이동되어 데이터 센터, 클라우드 컴퓨팅 또는 많은 CPU가 있는 모든 것에서 사용할 수 있게 되었음
- 데스크톱 Linux에 미치는 영향? 별로 없음

## How Discord Reduced Websocket Traffic by 40%

- 클라이언트가 Discord에 연결하면 "게이트웨이"라고 하는 서비스를 통해 무슨 일이 일어나고 있는지에 대한 실시간 업데이트를 받음
- 2017년 말부터 클라이언트의 게이트웨이 연결은 zlib를 사용하여 압축되어 메시지의 크기가 2배에서 10배까지 작아졌음
- Zstandard(zstd)는 zlib보다 압축률이 높고 압축 시간이 짧으며, 사전 기능을 지원해 대역폭을 추가로 줄일 수 있음
- 2019년 zstd 테스트 결과는 그다지 긍정적이지 않았으나, 다시 시도해볼 가치가 있다고 판단함
- Zstd 스트리밍
  - Zlib은 스트리밍 압축을 사용한 반면, zstd는 그렇지 않았음
  - 작은 페이로드에서 zstd가 zlib보다 성능이 떨어졌음
  - Elixir용 zstd 바인딩인 ezstd를 포크하여 스트리밍을 추가함
  - Zstd 스트리밍으로 전환한 결과 zlib 스트리밍보다 압축률과 속도 면에서 크게 개선됨
- 추가 성과: Passive Sessions V2
  - Zstd 적용 과정에서 passive_update_v1 메시지가 게이트웨이 대역폭의 30% 이상을 차지한다는 것을 발견함
  - 변경된 채널/멤버만 전송하는 passive_update_v2를 도입하여 해당 대역폭을 35%에서 5%로 줄임
- 큰 절감 효과
  - Passive Sessions v2와 zstd의 결합으로 클라이언트가 사용하는 게이트웨이 대역폭을 거의 40% 감소시킴
  - 의도치 않은 최적화 기회를 발견한 것은, 적절한 계측과 비판적 시각으로 그래프를 분석하는 것의 중요성을 보여줌

## The Intelligence Age

- AI가 가져올 엄청난 변화
  - 향후 몇 십 년 동안 우리 조상들이 마법처럼 여겼을 일들을 할 수 있게 될 것임
  - 이러한 현상은 새로운 것은 아니지만, AI로 인해 더욱 가속화될 것임
  - 유전적 변화 때문이 아니라 사회 기반 시설 자체가 우리 개개인보다 훨씬 더 똑똑하고 유능해졌기 때문에 인간의 능력이 크게 향상되었음
  - AI는 사람들에게 어려운 문제를 해결할 수 있는 도구를 제공하고, 우리 스스로는 알아낼 수 없었던 새로운 발전을 이룰 수 있게 도와줄 것임
- AI 시스템의 발전 전망
  - AI 모델은 곧 의료 관리를 조정하는 등 우리를 대신해 특정 작업을 수행하는 자율 개인 비서 역할을 할 것임
  - 향후 어느 시점에서는 AI 시스템이 너무 뛰어나져서 차세대 시스템을 만드는 데 도움을 주고 전반적인 과학 발전을 이룰 것임
  - 기술은 우리를 석기 시대에서 농경 시대로, 그리고 산업 시대로 이끌었음. 앞으로 지능 시대로 가는 길은 컴퓨팅, 에너지, 인간의 의지로 포장될 것임
- 밝은 미래에 대한 믿음
  - 미래는 너무나 밝아서 지금 그것에 대해 글을 쓰려고 해도 아무도 그것을 제대로 표현할 수 없을 것임
  - 지능 시대의 특징은 엄청난 번영이 될 것임
  - 점진적으로 일어나겠지만, 기후 문제 해결, 우주 식민지 건설, 모든 물리학의 발견과 같은 놀라운 승리는 결국 흔한 일이 될 것임
  - 거의 무한한 지능과 풍부한 에너지, 즉 위대한 아이디어를 만들어 내고 그것을 실현시킬 수 있는 능력이 있다면 우리는 상당히 많은 일을 할 수 있음
- AI의 위험 요소 최소화 필요성
  - 다른 기술에서 보았듯이 단점도 있을 것이며, AI의 이점을 최대화하고 해로운 점을 최소화하기 위해 지금부터 노력을 시작해야 함
  - 한 가지 예로, 이 기술이 향후 수년 내에 노동 시장에 상당한 변화(좋은 면과 나쁜 면)를 초래할 수 있을 것으로 예상되지만, 대부분의 일자리는 대부분의 사람들이 생각하는 것보다 더 천천히 변할 것이며, 우리가 할 일이 바닥날 것이라는 두려움은 없음

## Llama 3.2: Revolutionizing edge AI and vision with open, customizable models

- Meta, Llama 3.2 출시
  - 소형 및 중형 비전 LLM(11B 및 90B)과 경량 텍스트 전용 모델(1B 및 3B) 포함
  - Qualcomm 및 MediaTek 하드웨어에서 사용 가능하며 Arm 프로세서에 최적화됨
  - 요약, 지시 따르기, 재작성 작업에 적합한 모델
- Llama 3.2 모델의 특징
  - 11B 및 90B 비전 모델은 이미지 이해 작업에서 뛰어남
  - torchtune을 사용한 맞춤형 애플리케이션을 위한 미세 조정 가능
  - torchchat을 사용한 로컬 배포 가능
  - Meta AI 스마트 어시스턴트를 통해 사용 가능
- Llama 3.2의 성능
  - 11B 및 90B 모델은 문서 수준 이해, 이미지 캡션 작성, 시각적 기반 작업에 적합
  - 1B 및 3B 모델은 다국어 텍스트 생성 및 도구 호출 기능 제공
  - 로컬에서 실행 시 즉각적인 응답과 높은 프라이버시 유지

## API Impact Report 2024: AI Adoption and Innovation Challenges

- AI는 현재 기업에서 최우선 과제 중 하나이며, 이미 실질적인 비즈니스 가치를 창출하고 있음
- 92%가 AI를 우선순위로 꼽았고, 83%는 AI 투자로 지난 해 새로운 제품/서비스 기회를 만들었다고 응답함
- 기업의 숙제는 AI를 어떻게 빠르게 도입할 것인가에서, 보안과 컴플라이언스를 해치지 않으면서 AI를 어떻게 채택할 것인가로 바뀌고 있음
- 직원들의 일하는 방식과 고용주의 정책 간 간극으로 인해 정책과 모범사례가 무시되는 난맥상을 보이기도 함
- API가 AI 시대의 핵심 역할을 하며, API 관리 도구로 AI의 컴플라이언스와 보안을 담보할 수 있음
- Why are APIs essential in the AI age?
  - API는 AI가 작동하는데 필수불가결한 요소
  - 2026년까지 API 수요 증가의 30% 이상이 AI와 LLM 툴에서 나올 것으로 예측됨
  - API 관리에서 가장 중요한 측면은 AI 통합(19%)과 API 보안/거버넌스(15%)로 나타남
- The AI outlook
  - 60%는 AI에 대해 열정적이며 57%는 AI가 자신의 업무를 쉽게 만들 것이라 봄
  - 35%는 AI 도입이 너무 빠르다고 느끼며, 18%는 AI로 인한 정리해고 가능성을 우려함
  - 50%는 AI가 생산성과 혁신을 촉진할 것으로 보며, 26%는 AI로 새로운 직무나 기회가 만들어질 것으로 기대함
- Conclusion
  - 조직은 AI 혁신을 가능케 하되 보안과 컴플라이언스를 타협하지 않는 방안을 모색해야 함
  - AI와 API의 결합은 혁신의 길이자 경쟁우위 확보를 위한 핵심 전략임

## References

- https://www.anthropic.com/news/contextual-retrieval
- https://arstechnica.com/gadgets/2024/09/real-time-linux-is-officially-part-of-the-kernel-after-decades-of-debate/
- https://discord.com/blog/how-discord-reduced-websocket-traffic-by-40-percent
- https://ia.samaltman.com/
- https://ai.meta.com/blog/llama-3-2-connect-2024-vision-edge-mobile-devices/
- https://konghq.com/resources/reports/ai-and-api-adoption-challenges
