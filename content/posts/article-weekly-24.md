+++
date = 2024-06-19T03:30:00+09:00
title = "Article Weekly, Issue 24"
authors = ["Ji-Hoon Kim"]
tags = ["Article", "Weekly"]
categories = ["Article", "Weekly"]
series = ["Article Weekly"]
+++

## Period

`2024-06-09` ~ `2024-06-15`

## Beware! Anti-patterns in Event-Driven Architecture

- Leaking Internals
  - 각 서비스에 자체 저장소/데이터베이스가 있는 경우 서비스 경계 외부로 원하지 않는 스키마 및 데이터 유출에 대해 주의
- Commands
  - 게시-구독 패턴만 사용하고 도구가 이를 지원하는 경우 모든 것을 이벤트로 만드는 것이 쉬움
  - 그러나 사실은 그렇지 않음
  - 명령은 이벤트와 다름
    - 명령은 동작을 호출하는 반면 이벤트는 어떤 일이 발생했음을 알려줌
    - 명령은 요청으로 생각할 수 있고, 이벤트는 알림으로 생각할 수 있음
    - 명령의 결과는 종종 이벤트임
    - 명령에는 단일 소비자만 있고, 단일 소비자가 있어야 함
      - 게시-구독 패턴을 사용하지 않음
  - 원하는 패턴이 아닐 때 모든 것을 강제로 게시-구독하려고 하면 더 많은 패턴을 잘못 적용하게 됨
- Ordering
  - 메시지를 순서대로 처리하려면 동시성이 희생됨
  - 브로커는 파티션을 사용하여 이를 관리할 수 있음
    - 이는 파티션에 소비자가 하나만 있음을 의미
  - 해당 소비자는 둘 이상의 파티션을 담당할 수 있지만 파티션에는 소비자가 하나만 있음
    - 이를 통해 단일 소비자는 메시지를 순서대로 처리/소비할 수 있음
  - 메시지를 동시에 처리할 수단이 없기 때문에 규모와 처리량이 제한될 수 있음
  - 일반적으로 이는 경쟁 소비자의 패턴을 사용하여 수행됨
  - 순서가 지정된 이벤트 처리가 필요하다고 생각하지만 실제로는 그렇지 않은 상황이 많이 있음
  - 예를 들어 sagas를 사용하면 메시지를 순서대로 처리할 수 있음
  - 이는 실제로 현실 세계에서 상당히 흔한 일이며 실제로 비동기성을 수용하는 문제임
    - https://codeopinion.com/message-ordering-in-pub-sub-or-queues/
- Queries
  - 이벤트 기반 아키텍처와 비동기 메시징을 시작하면 모든 것을 이벤트로 바꾸고 싶어지기 시작
    - 그때부터 안티패턴에 빠지기 시작함
  - 특히 쿼리처럼 모든 것이 비동기적인 것은 아님
  - 쿼리는 그냥 요청-응답
  - 이벤트와 게시-구독을 사용하는 것은 이에 전혀 맞지 않음

## A Picture is Worth 170 Tokens: How Does GPT-4o Encode Images?

- GPT-4o는 고해상도 모드에서 사용되는 각 512x512 타일을 처리하는 데 170 토큰을 부과함. 약 0.75 토큰/단어의 비율로 보면 이는 그림 한 장이 약 227 단어와 같다는 것
- 170과 같은 숫자를 선택한 이유는 무엇일까?
- 이에 대한 많은 추측을 함

## What We’ve Learned From A Year of Building with LLMs

- 대규모 언어 모델(LLM)을 사용한 개발이 흥미로운 시기임
- AI로 구축하는 진입 장벽은 낮아졌지만, 데모 이상으로 효과적인 제품과 시스템을 만드는 것은 여전히 어려움
- 이 글은 세 부분으로 구성됨
  - Tactical (전술적)
    - 프롬프팅, RAG, 워크플로우 엔지니어링, 평가 및 모니터링을 위한 몇 가지 실천 사항
    - LLM으로 구축하는 실무자나 주말 프로젝트를 진행하는 사람들을 위해 작성됨
  - Operational (운영적)
    - 제품 출시의 조직적, 일상적 관심사와 효과적인 팀 구축 방법
    - 지속 가능하고 안정적으로 배포하려는 제품/기술 리더를 위한 내용
  - Strategic (전략적)
    - "PMF 전에 GPU 없음", "모델이 아닌 시스템에 집중" 등의 의견을 담은 장기적이고 큰 그림 관점과 반복하는 방법
    - 창업자와 경영진을 염두에 두고 작성됨
- 이 가이드는 LLM을 사용하여 성공적인 제품을 구축하기 위한 실용적인 안내서가 되는 것을 목표로 함

## What is PID 0?

- PID 0에 대해 웹에서 검색하면 대부분 잘못된 정보가 나옴
- Google, Bing, DuckDuckGo, Kagi에서 PID 0을 검색했을 때, 정확한 답변을 찾기 어려웠음
- PID 0은 스케줄링과 전원 관리에 관여하며, 페이징과는 관련이 없음
- PID 0은 CPU 코어가 할 일이 없을 때 실행되는 스케줄러 역할을 함
- 초기 Unix에서는 PID 0이 메모리 관리와 관련된 작업을 했으나, 현대 Unix에서는 그렇지 않음
- PID 0은 커널을 시작하고, 이후에는 CPU 코어를 관리하는 역할을 함
- Linux 커널에서 PID 0은 `do_idle` 함수로 구현되어 있음
- FreeBSD와 같은 다른 커널에서도 비슷한 역할을 수행함
- PID 0은 존재하며, 커널을 시작하는 스레드임
- PID 0은 초기 커널 초기화 작업을 수행하고, 이후에는 idle 스레드로 전환됨
- PID 0은 메모리 관리와는 관련이 없음
- 다중 코어 시스템에서는 각 코어마다 idle 스레드가 있으며, 이들은 모두 스레드 그룹 0에 속함

## Code Coverage Best Practices

- 코드 커버리지는 개발자 워크플로우에 많은 도움을 제공함
- 높은 코드 커버리지는 좋은 품질의 테스트 커버리지를 보장하지 않음
- 그러나 낮은 코드 커버리지는 제품의 많은 부분이 배포할 때마다 자동화에 의해 완전히 테스트되지 않는다는 것을 보장함
- 모든 제품에 적용되는 이상적인 코드 커버리지 숫자는 존재하지 않음
- 우리는 코드 커버리지가 90% ~ 95% 까지 도달하는 방법에 집착해서는 안됨
- 단위 테스트 코드 커버러지는 퍼즐의 한 조각일 뿐

## Performance Effects of Exceptions in Java

- 자바에서 예외는 일반적으로 비용이 많이 들기 때문에 흐름 제어에 사용되면 안됨
- 이에 대한 검증을 소개한 글

## AES-GCM and breaking it on nonce reuse

- AES-GCM 는 nonce 를 재사용하면 완전히 무너지게 되는데, 그 내용을 설명한 글

## encodings for flattened heap values

- 프로젝트 발할라의 핵심 목표
  - 가치 객체를 힙 컨테이너로 평평하게 만드는 동시에 해당 컨테이너의 모든 지정된 동작을 유지하는 것
  - 값의 플랫 표현은 값의 각 필드에 대해 저장 비트를 제공해야 하며, 컨테이너에 직접 할당되거나 적어도 간접적으로 액세스할 수 있어야 함
  - 참조 null 은 (컨테이너를 널링할 수 없는 경우) 표현 가능해야함
  - 레이싱 스레드가 컨테이너를 읽고 쓸 경우 컨테이너가 보장해야 하는 일관성 요구 사항도 종종 있음
- 이 문서에서는 힙 컨테이너에 대한 여러 가지 구현 옵션에 대해 설명
  - 우리는 컨테이너가 최대 64비트(또는 128비트)의 단일 기계어로 구성되는 흔하지만 어려운 플랫값 저장 사례에 특히 주목함
- 널 채널을 컨테이너에 (논리적으로) 저장해야 할 때 널 참조를 나타내기 위해 컨테이너에 도입하는 방법을 고려할 것
- 이 널 채널은 머신 워드의 내부에 있을 수도 있고, 머신 워드의 외부에 추가 바이트가 필요할 수도 있음

## How Meta trains large language models at scale

- Meta는 대규모 언어 모델(LLM) 학습을 위해 대규모 계산 능력이 필요함
- 전통적인 AI 모델 학습은 많은 수의 모델을 학습시켰지만, 비교적 적은 수의 GPU가 필요했음
- 생성형 AI(GenAI)의 등장으로 작업 수는 줄었지만, 매우 큰 작업이 필요해짐
- 대규모 모델 훈련의 도전 과제
  - 하드웨어 신뢰성
    - 하드웨어 고장으로 인한 훈련 중단을 최소화하기 위해 엄격한 테스트와 품질 관리 필요함.
  - 고장 시 빠른 복구
    - 하드웨어 고장이 발생하면 빠르게 복구할 수 있어야 함
    - 재스케줄링 오버헤드를 줄이고 빠른 훈련 재초기화 필요함
  - 훈련 상태의 효율적 보존
    - 고장 시 훈련 상태를 효율적으로 저장하고 복구할 수 있어야 함
  - GPU 간 최적의 연결성
    - 대규모 모델 훈련은 GPU 간 데이터 전송이 중요함
    - 이를 위해 고속 네트워크 인프라와 효율적인 데이터 전송 프로토콜 필요함

## References

- https://codeopinion.com/beware-anti-patterns-in-event-driven-architecture/
- https://www.oranlooney.com/post/gpt-cnn/
- https://applied-llms.org/
- https://blog.dave.tf/post/linux-pid0/
- https://testing.googleblog.com/2020/08/code-coverage-best-practices.html
- https://www.baeldung.com/java-exceptions-performance
- https://frereit.de/aes_gcm/
- https://cr.openjdk.org/~jrose/values/flattened-values.html
- https://engineering.fb.com/2024/06/12/data-infrastructure/training-large-language-models-at-scale-meta/
